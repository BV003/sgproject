# sgproject

## 项目原理概述
本项目旨在构建一个全面的三维场景理解与交互系统，该系统分为三个核心阶段，涵盖了从基础三维模型构建到语义信息融合，再到最终的交互式可视化与编辑的全过程。

### 第一阶段：三维场景基础模型生成(train)
此阶段专注于构建场景的几何与外观基础。首先，系统准备符合 COLMAP 标准格式的原始图像数据集，具体采用 Mip-NeRF 360 数据集，其中包含多视角图像以及通过结构光恢复技术获取的相机姿态与初始稀疏三维点云信息。随后，开发并运行定制化的 3DGS 训练模块。该模块的核心原理在于利用图像数据，通过迭代优化算法，逐步调整数百万个三维高斯体的各项参数。这些参数包括它们在三维空间中的精确位置、大小、旋转、不透明度，以及能够捕捉视角变化的球谐函数颜色系数。在优化过程中，还运用了致密化与剪枝等策略，以动态优化高斯体的分布和数量，从而确保渲染质量与效率达到最佳平衡。本阶段的最终输出是一个高质量的、可用于标准三维渲染的 .ply 格式的三维高斯体模型文件。

### 第二阶段：三维语义特征融合(fusion)
在获得基础的三维高斯体模型之后，此阶段的目标是为其赋予丰富的语义信息。在整个语义融合阶段，系统首先调用强大的预训练 2D 视觉-语言模型，例如 Segment Anything Model (SAM) 和 CLIP。这些模型在大规模 2D 图像数据上进行了充分训练，具备卓越的通用图像理解能力。系统目标是从输入场景的原始 2D 图像中提取像素级别的密集语义特征。这意味着对于图像中的每一个像素，系统不只是简单地获取一个离散的类别标签，而是得到一个高维的特征向量（例如 512 维或 768 维的特征嵌入）。这些特征向量能够捕捉到图像中每个点的丰富语义信息，包括物体的属性、类别乃至与语言概念的关联性，为实现开放词汇的理解奠定基础。

系统设计的核心融合机制，称之为多功能投影方法。该方法的关键在于其能够精确建立 2D 图像像素与 3D 高斯体之间的对应关系。具体而言，对于场景中的每一个三维高斯体，系统会利用其在三维空间中的位置、尺度、旋转以及渲染相机（包括相机的内参和外参），计算该高斯体在不同训练视角下如何投影到 2D 图像平面上。由于 3DGS 的渲染过程是可微分的，系统能够准确追踪到每一个像素是由哪些三维高斯体贡献而来的。

基于这种精确的 3D-2D 映射关系，系统会从每个高斯体在特定 2D 图像上对应的像素区域，提取出之前由 2D 视觉-语言模型生成的语义特征。例如，如果一个高斯体在某个视角下主要覆盖了 2D 图像中“树”的区域，那么它将从该区域获取到与“树”相关的语义特征。

更为关键的是，单个三维高斯体通常会在多个不同的训练图像视角中可见。为了获取一个更加鲁棒和一致的三维语义表示，系统会对这些来自多个视角的语义特征进行聚合。系统采用加权平均的方式，将每个高斯体在不同视图下获得的语义特征累加起来，然后根据该高斯体在所有视图中被看到的次数进行平均。通过这种多视角聚合策略，可以有效地消除单一视角可能带来的噪声或模糊性，从而为每个三维高斯体生成一个更稳定、更精确的高维语义特征向量。这些特征向量不仅仅是简单的颜色信息，它们能够深层次地编码高斯体所代表的三维区域的语义内容，使其能够被高级语义查询和理解所利用。

本阶段工作完成后，系统将得到一个 .pt 格式的文件。这个文件是 PyTorch 的序列化格式，它精确地保存了所有三维高斯体经过上述融合过程后获得的高维语义特征。这些特征向量与原始 3DGS 模型文件（.ply 文件）中的高斯体数据严格对应，即 .pt 文件中的第 N 个特征向量对应着 .ply 文件中的第 N 个高斯体。这确保了在后续的前端可视化和语义编辑环节中，能够无缝地将几何外观与语义信息关联起来。

### 第三阶段：交互式三维语义可视化与编辑(view)
这是整个项目面向用户和应用的核心阶段。首先，系统从磁盘加载预训练的 3D 高斯点云模型，支持 .ply 或 .npz 格式。系统会提取高斯参数，包括颜色、位置、尺度和透明度，并根据时间戳区分静态与动态场景的多帧点云。同时，系统会载入预先计算的语义嵌入向量，例如 CLIP 或 OpenSeg 特征，并将语义特征和掩码存储至 GPU。随后，系统基于用户指定的 2D 语义编码模型，如 OpenSeg 或 LSeg，提取文本特征。

系统构建了 viser Web 交互式前端，启动支持多客户端连接的 viser 服务器，并提供可视化 GUI 界面。该界面包含丰富的控制选项，支持 RGB、深度图、语义图、语义相关性等渲染模式切换，可调节模型缩放与分辨率，提供文本提示输入框用于语义匹配与上色，还支持按语义进行删除、染色、尺寸调整或移动等编辑操作。

在语义匹配与点编辑模块中，系统通过 CLIP 文本嵌入与高斯点语义嵌入的余弦相似度计算，生成软标签与硬标签，并据此为每个高斯点赋予颜色以生成语义分布用于渲染。若启用编辑模式，用户可以执行特定操作，包括删除指定语义点、反色处理、调整点的位置或尺寸，同时支持锁定特定语义类别点以避免误编辑。

渲染系统支持多种模式实时响应前端操作。RGB 模式显示原始纹理颜色，Depth 模式以灰度可视化深度图，Semantic 模式通过伪彩色展示语义分类，Relevancy 模式用颜色表征与语义的相似程度，且渲染分辨率与缩放因子均可按需调节。

---

## 客户端 GUI 界面与功能概览

### 1. **标签页（Tab Group）**

* **Settings（设置）标签页**

  * **Render mode（渲染模式）按钮组**：用户可选择渲染类型

    * RGB 彩色渲染
    * Depth 深度图
    * Semantic 语义分割图
    * Relevancy 语义相关度图
  * **Depth near / Depth far（深度范围滑条）**：调整深度图的近远裁剪范围，影响深度图视觉效果
  * **Gaussian scale（高斯点大小滑条）**：调节点云中高斯体积的缩放比例，影响点云显示大小
  * **Resolution scale（分辨率缩放按钮组）**：选择渲染输出分辨率倍率（0.5x，1x，2x，4x），调整画面清晰度与性能
  * **Lock up direction（锁定方向复选框）**：是否锁定相机的上方向，帮助固定视角
  * **Text prompt（文本提示输入框）**：输入自然语言关键词（用逗号分割），告诉系统你想重点显示哪些语义类别
  * **Apply text prompt（应用文本提示按钮）**：点击后，界面会根据文本内容重新计算语义高亮和渲染

---

### 2. **Editing（编辑）标签页**

* **Edit mode（编辑模式按钮组）**：

  * Remove：删除选中语义的点云高斯
  * Color：改变选中语义的颜色
  * Size：调整选中语义的高斯体积大小
  * Move：移动选中语义的点云位置
* **Edit prompt（编辑语义关键词输入框）**：输入需要操作的语义类别关键词（逗号分割）
* **Preserve prompt（保留语义关键词输入框）**：输入想要保留不受影响的类别关键词
* **Apply editing prompt（应用编辑命令按钮）**：执行编辑操作，修改点云显示效果

---

### 3. **Colormap（颜色映射）标签页**

* 显示当前语义类别对应的颜色表，方便用户了解不同类别颜色对应的语义意义

---

### 4. **交互体验**

* **多客户端支持**：多个用户可同时连接该服务，独立控制各自视角和操作
* **相机交互**：用户可通过鼠标/手势旋转、缩放、平移视角，实时观察三维模型
* **实时渲染更新**：当用户调整渲染模式、编辑操作或文本提示后，页面画面即时刷新，呈现最新效果
* **动态参数调整**：滑条和按钮的调整直接影响渲染细节和视觉效果，操作直观易用

---

## 简单总结

| GUI 组成        | 用户操作及功能描述                                             |
| --------------- | -------------------------------------------------------------- |
| Settings 标签页 | 选择渲染模式、调节深度范围、点大小、分辨率，输入关键词高亮语义 |
| Editing 标签页  | 选择编辑类型，输入关键词删除/变色/移动/缩放点云语义元素        |
| Colormap 标签页 | 查看语义类别对应的颜色说明                                     |
| 视角交互        | 通过鼠标或手势实时控制观察角度                                 |
| 多用户支持      | 多个客户端独立连接、控制和渲染                                 |


## Install
你可以选择自己创建环境，也可以选择用docker拉取镜像
1. Create individual virtual environment (or use existing environments with CUDA Development kit and corresponding version of PyTorch).
    ```bash
    conda env create -f environment.yaml
    conda activate sega
    ```

2. Install additional dependencies with pip as many of them need to be compiled.
    ```bash
    pip install -r requirements.txt
    ```

3. Compile and install MinkowskiEngine through anaconda, recommending to install through official instructions.
    ```bash
    # Here is an example only for Anaconda, CUDA 11.x
    conda install openblas-devel -c anaconda
    pip install git+https://github.com/NVIDIA/MinkowskiEngine -v --no-deps --install-option="--blas_include_dirs=${CONDA_PREFIX}/include" --install-option="--blas=openblas"
    ```

4.  拉取docker镜像，并从docker镜像启动容器
    ```bash
    docker run -it \-p 8080:8080 \
    --gpus all \
    --shm-size=24g \
    -v ~/Desktop/liuweiqi/Data:/workspace/Data \
    -v ~/Desktop/liuweiqi/sgproject:/workspace/sgproject \
    --name sgcontainer \
    sgproject:v1.0 /bin/bash

    ```